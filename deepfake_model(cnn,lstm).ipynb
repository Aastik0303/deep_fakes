{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB8mW-QCe80B",
        "outputId": "9703311c-ec86-47f3-87d9-ca232623c9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/unidpro/deepfake-videos-dataset\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w9zTErMfCxP",
        "outputId": "da9ade79-44ec-4d51-90d8-a4a8144ea304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: aastikmishra03\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/unidpro/deepfake-videos-dataset\n",
            "Downloading deepfake-videos-dataset.zip to ./deepfake-videos-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64.6M/64.6M [00:00<00:00, 1.42GB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e383d937",
        "outputId": "6f802a1b-1719-42e4-bfd1-351c94652473"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the downloaded dataset directory\n",
        "dataset_path = './deepfake-videos-dataset'\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Contents of '{dataset_path}':\")\n",
        "    for item in os.listdir(dataset_path):\n",
        "        print(item)\n",
        "else:\n",
        "    print(f\"Dataset directory '{dataset_path}' not found.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of './deepfake-videos-dataset':\n",
            "image\n",
            "deepfake\n",
            "DeepFake Videos Dataset.csv\n",
            "video\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "b7134be3",
        "outputId": "2b68f5eb-92c8-4c1c-98ea-bd0815911c73"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = os.path.join(dataset_path, 'DeepFake Videos Dataset.csv')\n",
        "\n",
        "if os.path.exists(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"First 5 rows of the dataframe:\")\n",
        "    display(df.head())\n",
        "    print(\"\\nInformation about the dataframe:\")\n",
        "    df.info()\n",
        "else:\n",
        "    print(f\"CSV file not found at '{csv_path}'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the dataframe:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   id        deepfake         image        video\n",
              "0   1  deepfake/1.mp4   image/1.jpg  video/1.mp4\n",
              "1   2  deepfake/2.mp4  image/2.jpeg  video/2.mp4\n",
              "2   3  deepfake/3.mp4   image/3.jpg  video/3.mp4\n",
              "3   4  deepfake/4.mp4   image/4.jpg  video/4.mp4\n",
              "4   5  deepfake/5.mov   image/5.jpg  video/5.mov"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-949683ae-2337-43fd-89f9-e392179ee73e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>deepfake</th>\n",
              "      <th>image</th>\n",
              "      <th>video</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>deepfake/1.mp4</td>\n",
              "      <td>image/1.jpg</td>\n",
              "      <td>video/1.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>deepfake/2.mp4</td>\n",
              "      <td>image/2.jpeg</td>\n",
              "      <td>video/2.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>deepfake/3.mp4</td>\n",
              "      <td>image/3.jpg</td>\n",
              "      <td>video/3.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>deepfake/4.mp4</td>\n",
              "      <td>image/4.jpg</td>\n",
              "      <td>video/4.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>deepfake/5.mov</td>\n",
              "      <td>image/5.jpg</td>\n",
              "      <td>video/5.mov</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-949683ae-2337-43fd-89f9-e392179ee73e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-949683ae-2337-43fd-89f9-e392179ee73e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-949683ae-2337-43fd-89f9-e392179ee73e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4f04112f-0225-4e53-a9dc-6e1462cfb618\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f04112f-0225-4e53-a9dc-6e1462cfb618')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4f04112f-0225-4e53-a9dc-6e1462cfb618 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"CSV file not found at '{csv_path}'\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deepfake\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"deepfake/2.mp4\",\n          \"deepfake/5.mov\",\n          \"deepfake/3.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"image/2.jpeg\",\n          \"image/5.jpg\",\n          \"image/3.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"video/2.mp4\",\n          \"video/5.mov\",\n          \"video/3.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Information about the dataframe:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        5 non-null      int64 \n",
            " 1   deepfake  5 non-null      object\n",
            " 2   image     5 non-null      object\n",
            " 3   video     5 non-null      object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 292.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ecede1e"
      },
      "source": [
        "# Task\n",
        "Analyze the provided dataset containing video files and corresponding deepfake information to understand its structure and characteristics, and prepare the data for further analysis or modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1e88feb"
      },
      "source": [
        "## Understand the dataset structure\n",
        "\n",
        "### Subtask:\n",
        "Analyze the columns in the DataFrame to understand how the video files, images, and deepfake information are linked.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87fead2e"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the previous outputs of `df.head()` and `df.info()`, I will describe the purpose of each column and explain the relationship between the 'deepfake', 'image', and 'video' columns to fulfill the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12db252c",
        "outputId": "d2ecc9e5-ec0d-49ad-8d1e-f6bbea55a467"
      },
      "source": [
        "# Describe the purpose of each column\n",
        "print(\"Column Descriptions:\")\n",
        "print(\"- 'id': A unique identifier for each entry/sample in the dataset (integer type).\")\n",
        "print(\"- 'deepfake': The file path or name of the deepfake video corresponding to the entry (object/string type).\")\n",
        "print(\"- 'image': The file path or name of an image associated with the entry, possibly a frame from the original or deepfake video, or a related image (object/string type).\")\n",
        "print(\"- 'video': The file path or name of the original video corresponding to the entry (object/string type).\")\n",
        "\n",
        "# Explain the relationship between 'deepfake', 'image', and 'video'\n",
        "print(\"\\nRelationship between 'deepfake', 'image', and 'video' columns:\")\n",
        "print(\"For each row in the dataset, the 'id' links together three related files:\")\n",
        "print(\"- The 'video' column provides the path to the original video file.\")\n",
        "print(\"- The 'deepfake' column provides the path to the manipulated (deepfake) version of that original video.\")\n",
        "print(\"- The 'image' column provides the path to an image related to this pair of videos, likely a representative frame or a source image used in the deepfake creation process.\")\n",
        "print(\"Each row represents a single sample, where the 'id' serves as a key to associate the original video, its deepfake counterpart, and a related image.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Descriptions:\n",
            "- 'id': A unique identifier for each entry/sample in the dataset (integer type).\n",
            "- 'deepfake': The file path or name of the deepfake video corresponding to the entry (object/string type).\n",
            "- 'image': The file path or name of an image associated with the entry, possibly a frame from the original or deepfake video, or a related image (object/string type).\n",
            "- 'video': The file path or name of the original video corresponding to the entry (object/string type).\n",
            "\n",
            "Relationship between 'deepfake', 'image', and 'video' columns:\n",
            "For each row in the dataset, the 'id' links together three related files:\n",
            "- The 'video' column provides the path to the original video file.\n",
            "- The 'deepfake' column provides the path to the manipulated (deepfake) version of that original video.\n",
            "- The 'image' column provides the path to an image related to this pair of videos, likely a representative frame or a source image used in the deepfake creation process.\n",
            "Each row represents a single sample, where the 'id' serves as a key to associate the original video, its deepfake counterpart, and a related image.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69a8dd2b"
      },
      "source": [
        "## Explore video file paths\n",
        "\n",
        "### Subtask:\n",
        "Inspect the file paths provided in the DataFrame to confirm their structure and existence within the downloaded directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9994129f"
      },
      "source": [
        "**Reasoning**:\n",
        "Access the first few rows of the relevant columns, construct full paths for a few examples, and check their existence using `os.path.exists()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "849f8176",
        "outputId": "f7f98f52-26a2-403d-bb4b-1cce665a830a"
      },
      "source": [
        "# 1. Access the first few rows of the 'video', 'deepfake', and 'image' columns\n",
        "print(\"First few rows of 'video', 'deepfake', and 'image' columns:\")\n",
        "display(df[['video', 'deepfake', 'image']].head())\n",
        "\n",
        "# 2. Construct the full file paths for a few examples\n",
        "# Assuming the first row is representative for constructing paths\n",
        "example_video_path_rel = df['video'].iloc[0]\n",
        "example_deepfake_path_rel = df['deepfake'].iloc[0]\n",
        "example_image_path_rel = df['image'].iloc[0]\n",
        "\n",
        "example_video_path_full = os.path.join(dataset_path, example_video_path_rel)\n",
        "example_deepfake_path_full = os.path.join(dataset_path, example_deepfake_path_rel)\n",
        "example_image_path_full = os.path.join(dataset_path, example_image_path_rel)\n",
        "\n",
        "print(f\"\\nConstructed full paths (example from first row):\")\n",
        "print(f\"Video: {example_video_path_full}\")\n",
        "print(f\"Deepfake: {example_deepfake_path_full}\")\n",
        "print(f\"Image: {example_image_path_full}\")\n",
        "\n",
        "# 3. Use the os.path.exists() function to check if these constructed file paths exist\n",
        "video_exists = os.path.exists(example_video_path_full)\n",
        "deepfake_exists = os.path.exists(example_deepfake_path_full)\n",
        "image_exists = os.path.exists(example_image_path_full)\n",
        "\n",
        "# 4. Print the results of the existence checks\n",
        "print(\"\\nExistence check results:\")\n",
        "print(f\"Video path '{example_video_path_full}' exists: {video_exists}\")\n",
        "print(f\"Deepfake path '{example_deepfake_path_full}' exists: {deepfake_exists}\")\n",
        "print(f\"Image path '{example_image_path_full}' exists: {image_exists}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of 'video', 'deepfake', and 'image' columns:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         video        deepfake         image\n",
              "0  video/1.mp4  deepfake/1.mp4   image/1.jpg\n",
              "1  video/2.mp4  deepfake/2.mp4  image/2.jpeg\n",
              "2  video/3.mp4  deepfake/3.mp4   image/3.jpg\n",
              "3  video/4.mp4  deepfake/4.mp4   image/4.jpg\n",
              "4  video/5.mov  deepfake/5.mov   image/5.jpg"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de365fae-3fb2-4686-9568-944d3de49116\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video</th>\n",
              "      <th>deepfake</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>video/1.mp4</td>\n",
              "      <td>deepfake/1.mp4</td>\n",
              "      <td>image/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>video/2.mp4</td>\n",
              "      <td>deepfake/2.mp4</td>\n",
              "      <td>image/2.jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>video/3.mp4</td>\n",
              "      <td>deepfake/3.mp4</td>\n",
              "      <td>image/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>video/4.mp4</td>\n",
              "      <td>deepfake/4.mp4</td>\n",
              "      <td>image/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>video/5.mov</td>\n",
              "      <td>deepfake/5.mov</td>\n",
              "      <td>image/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de365fae-3fb2-4686-9568-944d3de49116')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-de365fae-3fb2-4686-9568-944d3de49116 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-de365fae-3fb2-4686-9568-944d3de49116');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-247ce5b6-b146-4e67-b1d2-a154437943c6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-247ce5b6-b146-4e67-b1d2-a154437943c6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-247ce5b6-b146-4e67-b1d2-a154437943c6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(f\\\"Image path '{example_image_path_full}' exists: {image_exists}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"video\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"video/2.mp4\",\n          \"video/5.mov\",\n          \"video/3.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deepfake\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"deepfake/2.mp4\",\n          \"deepfake/5.mov\",\n          \"deepfake/3.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"image/2.jpeg\",\n          \"image/5.jpg\",\n          \"image/3.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Constructed full paths (example from first row):\n",
            "Video: ./deepfake-videos-dataset/video/1.mp4\n",
            "Deepfake: ./deepfake-videos-dataset/deepfake/1.mp4\n",
            "Image: ./deepfake-videos-dataset/image/1.jpg\n",
            "\n",
            "Existence check results:\n",
            "Video path './deepfake-videos-dataset/video/1.mp4' exists: True\n",
            "Deepfake path './deepfake-videos-dataset/deepfake/1.mp4' exists: True\n",
            "Image path './deepfake-videos-dataset/image/1.jpg' exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8e2f8f1"
      },
      "source": [
        "## Visualize example videos\n",
        "\n",
        "### Subtask:\n",
        "Select a few examples of both deepfake and original videos and display them. This might require installing a video player or using a library to extract frames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e75a66d"
      },
      "source": [
        "**Reasoning**:\n",
        "Displaying video directly in a Jupyter environment can be challenging due to format compatibility and resource usage. The dataset contains only 5 rows, so we can display the paths for all of them and provide instructions for external viewing. This is the most reliable approach given the constraints and available information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dec3c7d",
        "outputId": "795f6990-0278-42e7-a186-c1ae6378a5c2"
      },
      "source": [
        "import os\n",
        "\n",
        "# Select all rows as examples since the dataset is small\n",
        "example_videos_df = df\n",
        "\n",
        "print(\"Selected examples for external viewing:\")\n",
        "print(\"\\nOriginal Videos:\")\n",
        "for index, row in example_videos_df.iterrows():\n",
        "    original_video_path_rel = row['video']\n",
        "    original_video_path_full = os.path.join(dataset_path, original_video_path_rel)\n",
        "    print(f\"ID {row['id']}: {original_video_path_full}\")\n",
        "\n",
        "print(\"\\nDeepfake Videos:\")\n",
        "for index, row in example_videos_df.iterrows():\n",
        "    deepfake_video_path_rel = row['deepfake']\n",
        "    deepfake_video_path_full = os.path.join(dataset_path, deepfake_video_path_rel)\n",
        "    print(f\"ID {row['id']}: {deepfake_video_path_full}\")\n",
        "\n",
        "print(\"\\nTo view these videos, copy the full path and paste it into a media player on your system (e.g., VLC Media Player, Windows Media Player, QuickTime).\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected examples for external viewing:\n",
            "\n",
            "Original Videos:\n",
            "ID 1: ./deepfake-videos-dataset/video/1.mp4\n",
            "ID 2: ./deepfake-videos-dataset/video/2.mp4\n",
            "ID 3: ./deepfake-videos-dataset/video/3.mp4\n",
            "ID 4: ./deepfake-videos-dataset/video/4.mp4\n",
            "ID 5: ./deepfake-videos-dataset/video/5.mov\n",
            "\n",
            "Deepfake Videos:\n",
            "ID 1: ./deepfake-videos-dataset/deepfake/1.mp4\n",
            "ID 2: ./deepfake-videos-dataset/deepfake/2.mp4\n",
            "ID 3: ./deepfake-videos-dataset/deepfake/3.mp4\n",
            "ID 4: ./deepfake-videos-dataset/deepfake/4.mp4\n",
            "ID 5: ./deepfake-videos-dataset/deepfake/5.mov\n",
            "\n",
            "To view these videos, copy the full path and paste it into a media player on your system (e.g., VLC Media Player, Windows Media Player, QuickTime).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bf47c4e"
      },
      "source": [
        "## Analyze video characteristics\n",
        "\n",
        "### Subtask:\n",
        "Analyze characteristics of the videos such as resolution, duration, and frame rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccd0fee1"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the cv2 library to analyze video characteristics and iterate through the video file paths to extract features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dc52c63",
        "outputId": "ac1ea1e0-7c26-4148-9860-0e31c1604b05"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "print(\"Analyzing video characteristics (resolution, frame rate, duration):\")\n",
        "\n",
        "# Iterate through the video file paths\n",
        "for index, row in df.iterrows():\n",
        "    original_video_path_rel = row['video']\n",
        "    deepfake_video_path_rel = row['deepfake']\n",
        "\n",
        "    original_video_path_full = os.path.join(dataset_path, original_video_path_rel)\n",
        "    deepfake_video_path_full = os.path.join(dataset_path, deepfake_video_path_rel)\n",
        "\n",
        "    print(f\"\\n--- Analyzing Video ID: {row['id']} ---\")\n",
        "\n",
        "    # Analyze Original Video\n",
        "    print(\"  Original Video:\")\n",
        "    cap_original = cv2.VideoCapture(original_video_path_full)\n",
        "\n",
        "    if not cap_original.isOpened():\n",
        "        print(f\"    Error: Could not open original video file {original_video_path_full}\")\n",
        "    else:\n",
        "        frame_width_orig = int(cap_original.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height_orig = int(cap_original.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps_orig = cap_original.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count_orig = cap_original.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "        duration_orig = 0\n",
        "        if fps_orig > 0:\n",
        "            duration_orig = frame_count_orig / fps_orig\n",
        "        else:\n",
        "            print(\"    Warning: Original video has zero frame rate, duration cannot be calculated.\")\n",
        "\n",
        "        print(f\"    Resolution: {frame_width_orig}x{frame_height_orig}\")\n",
        "        print(f\"    Frame Rate (FPS): {fps_orig:.2f}\")\n",
        "        print(f\"    Duration (seconds): {duration_orig:.2f}\")\n",
        "\n",
        "        cap_original.release()\n",
        "\n",
        "    # Analyze Deepfake Video\n",
        "    print(\"  Deepfake Video:\")\n",
        "    cap_deepfake = cv2.VideoCapture(deepfake_video_path_full)\n",
        "\n",
        "    if not cap_deepfake.isOpened():\n",
        "        print(f\"    Error: Could not open deepfake video file {deepfake_video_path_full}\")\n",
        "    else:\n",
        "        frame_width_dfake = int(cap_deepfake.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height_dfake = int(cap_deepfake.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps_dfake = cap_deepfake.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count_dfake = cap_deepfake.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "        duration_dfake = 0\n",
        "        if fps_dfake > 0:\n",
        "            duration_dfake = frame_count_dfake / fps_dfake\n",
        "        else:\n",
        "             print(\"    Warning: Deepfake video has zero frame rate, duration cannot be calculated.\")\n",
        "\n",
        "        print(f\"    Resolution: {frame_width_dfake}x{frame_height_dfake}\")\n",
        "        print(f\"    Frame Rate (FPS): {fps_dfake:.2f}\")\n",
        "        print(f\"    Duration (seconds): {duration_dfake:.2f}\")\n",
        "\n",
        "        cap_deepfake.release()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing video characteristics (resolution, frame rate, duration):\n",
            "\n",
            "--- Analyzing Video ID: 1 ---\n",
            "  Original Video:\n",
            "    Resolution: 720x1280\n",
            "    Frame Rate (FPS): 20.00\n",
            "    Duration (seconds): 9.90\n",
            "  Deepfake Video:\n",
            "    Resolution: 720x1280\n",
            "    Frame Rate (FPS): 20.00\n",
            "    Duration (seconds): 10.00\n",
            "\n",
            "--- Analyzing Video ID: 2 ---\n",
            "  Original Video:\n",
            "    Resolution: 1080x1920\n",
            "    Frame Rate (FPS): 29.99\n",
            "    Duration (seconds): 10.80\n",
            "  Deepfake Video:\n",
            "    Resolution: 1080x1920\n",
            "    Frame Rate (FPS): 30.00\n",
            "    Duration (seconds): 4.50\n",
            "\n",
            "--- Analyzing Video ID: 3 ---\n",
            "  Original Video:\n",
            "    Resolution: 1080x1920\n",
            "    Frame Rate (FPS): 30.12\n",
            "    Duration (seconds): 9.20\n",
            "  Deepfake Video:\n",
            "    Resolution: 288x512\n",
            "    Frame Rate (FPS): 24.00\n",
            "    Duration (seconds): 9.21\n",
            "\n",
            "--- Analyzing Video ID: 4 ---\n",
            "  Original Video:\n",
            "    Resolution: 720x1280\n",
            "    Frame Rate (FPS): 30.01\n",
            "    Duration (seconds): 8.40\n",
            "  Deepfake Video:\n",
            "    Resolution: 288x512\n",
            "    Frame Rate (FPS): 24.00\n",
            "    Duration (seconds): 8.42\n",
            "\n",
            "--- Analyzing Video ID: 5 ---\n",
            "  Original Video:\n",
            "    Error: Could not open original video file ./deepfake-videos-dataset/video/5.mov\n",
            "  Deepfake Video:\n",
            "    Resolution: 360x480\n",
            "    Frame Rate (FPS): 24.00\n",
            "    Duration (seconds): 10.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "613dbfb4"
      },
      "source": [
        "## Preprocessing (if necessary)\n",
        "\n",
        "### Subtask:\n",
        "Based on the analysis and the specific task (e.g., training a deepfake detection model), determine if any preprocessing steps are needed for the video data (e.g., frame extraction, resizing, normalization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "054743bc"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the analysis showing variations in video characteristics (resolution, frame rate, duration) and the inability to read one video file, preprocessing steps are necessary for a deepfake detection model. These steps would typically include frame extraction, resizing frames to a uniform size, and potentially addressing the unreadable file issue. Given the small dataset size, this preprocessing is outlined as a necessary step before any potential model training, while noting the limitations imposed by the dataset size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bf7cd70",
        "outputId": "0bf70888-e195-4698-c125-fd42fbcaab36"
      },
      "source": [
        "print(\"Preprocessing Considerations for Deepfake Detection Model:\")\n",
        "\n",
        "# 1. Frame Extraction:\n",
        "print(\"\\n1. Frame Extraction:\")\n",
        "print(\"   - Deepfake detection models often process individual frames or sequences of frames.\")\n",
        "print(\"   - This requires extracting frames from each video file.\")\n",
        "print(\"   - For this dataset, we would iterate through each video (original and deepfake) and save its frames as image files.\")\n",
        "\n",
        "# 2. Resizing:\n",
        "print(\"\\n2. Resizing Frames:\")\n",
        "print(\"   - The analysis showed variations in video resolution (e.g., 720x1280 vs 360x480).\")\n",
        "print(\"   - Machine learning models typically require input data of a fixed size.\")\n",
        "print(\"   - Therefore, extracted frames would need to be resized to a uniform resolution (e.g., 256x256 or 224x224) before being fed into a model.\")\n",
        "\n",
        "# 3. Handling Unreadable Video File:\n",
        "print(\"\\n3. Handling Unreadable Video File:\")\n",
        "print(\"   - The analysis identified one original video file (.mov) that could not be opened with OpenCV.\")\n",
        "print(\"   - This file and its corresponding deepfake would need to be excluded from the dataset or the issue with reading the file resolved (e.g., using a different library or converting the file format).\")\n",
        "print(\"   - Given the small dataset, excluding a sample significantly reduces the data available for training.\")\n",
        "\n",
        "# 4. Standardizing Frame Rate:\n",
        "print(\"\\n4. Standardizing Frame Rate (Optional but Recommended):\")\n",
        "print(\"   - Variations in frame rate were observed.\")\n",
        "print(\"   - For sequence-based models, a consistent frame rate or a strategy to handle varying frame rates (e.g., selecting a fixed number of frames or interpolating) might be necessary.\")\n",
        "\n",
        "# 5. Data Augmentation:\n",
        "print(\"\\n5. Data Augmentation:\")\n",
        "print(\"   - Given the very small size of this dataset (5 samples), data augmentation techniques (e.g., random cropping, flipping, rotation) on the extracted frames or video sequences would be crucial to increase the effective dataset size and improve model generalization.\")\n",
        "\n",
        "# 6. Limitations due to Dataset Size:\n",
        "print(\"\\nLimitations due to Dataset Size:\")\n",
        "print(\"   - This dataset contains only 5 samples, which is extremely small for training a robust deepfake detection model.\")\n",
        "print(\"   - While preprocessing steps like frame extraction and resizing are necessary prerequisites, training a meaningful model with such limited data would be highly challenging and likely result in poor performance and generalization.\")\n",
        "print(\"   - The primary value of this dataset lies in understanding the data structure and practicing initial data loading and analysis steps, rather than building a production-ready model.\")\n",
        "\n",
        "print(\"\\nConclusion: Preprocessing steps including frame extraction, resizing, and handling unreadable files are necessary for preparing this data for a deepfake detection model. However, the extremely small dataset size poses a significant limitation for effective model training.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Considerations for Deepfake Detection Model:\n",
            "\n",
            "1. Frame Extraction:\n",
            "   - Deepfake detection models often process individual frames or sequences of frames.\n",
            "   - This requires extracting frames from each video file.\n",
            "   - For this dataset, we would iterate through each video (original and deepfake) and save its frames as image files.\n",
            "\n",
            "2. Resizing Frames:\n",
            "   - The analysis showed variations in video resolution (e.g., 720x1280 vs 360x480).\n",
            "   - Machine learning models typically require input data of a fixed size.\n",
            "   - Therefore, extracted frames would need to be resized to a uniform resolution (e.g., 256x256 or 224x224) before being fed into a model.\n",
            "\n",
            "3. Handling Unreadable Video File:\n",
            "   - The analysis identified one original video file (.mov) that could not be opened with OpenCV.\n",
            "   - This file and its corresponding deepfake would need to be excluded from the dataset or the issue with reading the file resolved (e.g., using a different library or converting the file format).\n",
            "   - Given the small dataset, excluding a sample significantly reduces the data available for training.\n",
            "\n",
            "4. Standardizing Frame Rate (Optional but Recommended):\n",
            "   - Variations in frame rate were observed.\n",
            "   - For sequence-based models, a consistent frame rate or a strategy to handle varying frame rates (e.g., selecting a fixed number of frames or interpolating) might be necessary.\n",
            "\n",
            "5. Data Augmentation:\n",
            "   - Given the very small size of this dataset (5 samples), data augmentation techniques (e.g., random cropping, flipping, rotation) on the extracted frames or video sequences would be crucial to increase the effective dataset size and improve model generalization.\n",
            "\n",
            "Limitations due to Dataset Size:\n",
            "   - This dataset contains only 5 samples, which is extremely small for training a robust deepfake detection model.\n",
            "   - While preprocessing steps like frame extraction and resizing are necessary prerequisites, training a meaningful model with such limited data would be highly challenging and likely result in poor performance and generalization.\n",
            "   - The primary value of this dataset lies in understanding the data structure and practicing initial data loading and analysis steps, rather than building a production-ready model.\n",
            "\n",
            "Conclusion: Preprocessing steps including frame extraction, resizing, and handling unreadable files are necessary for preparing this data for a deepfake detection model. However, the extremely small dataset size poses a significant limitation for effective model training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab20f851"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset contains four columns: 'id', 'deepfake', 'image', and 'video'.\n",
        "*   Each row in the dataset represents a single sample, where the 'id' links together the original video ('video'), its corresponding deepfake version ('deepfake'), and a related image file ('image').\n",
        "*   The file paths provided in the DataFrame for 'video', 'deepfake', and 'image' columns correspond to actual files within the specified dataset directory.\n",
        "*   Analysis of video characteristics using OpenCV revealed variations in resolution, frame rate, and duration between original and deepfake video pairs.\n",
        "*   One original video file (.mov) could not be opened by OpenCV during the analysis.\n",
        "*   The dataset contains only 5 samples, which is extremely small for training a robust deepfake detection model.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Preprocessing steps including frame extraction, resizing, handling unreadable files, and potentially standardizing frame rate are necessary to prepare this data for use in a deepfake detection model.\n",
        "*   Given the extremely limited size of the dataset (5 samples), data augmentation would be crucial if attempting to train a model; however, the primary value of this dataset is for understanding data structure and practicing initial analysis steps, rather than building a production-ready model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e50a64ce"
      },
      "source": [
        "# Task\n",
        "Analyze the provided video files to classify them as original or deepfake using CNN and LSTM models. The analysis should include preprocessing the videos, preparing the data for the models, building and training the models, and evaluating their performance. The video files are located at \"deepfake_and_original_videos\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43f18b58"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Extract frames from the videos, resize them to a uniform size, and handle the unreadable video file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aea79dd"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract frames from the videos, resize them, and save them to organized directories while handling the unreadable video file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d0da8e7",
        "outputId": "5de6de39-815b-4b87-d8d7-894b06f3aa62"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# 1. Set a target image size for resizing the extracted frames\n",
        "IMG_SIZE = (56, 56)\n",
        "\n",
        "# 2. Create a directory structure to store the extracted frames\n",
        "original_frames_dir = os.path.join(dataset_path, 'extracted_frames', 'original')\n",
        "deepfake_frames_dir = os.path.join(dataset_path, 'extracted_frames', 'deepfake')\n",
        "\n",
        "os.makedirs(original_frames_dir, exist_ok=True)\n",
        "os.makedirs(deepfake_frames_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Created directories for extracted frames:\")\n",
        "print(f\"- Original: {original_frames_dir}\")\n",
        "print(f\"- Deepfake: {deepfake_frames_dir}\")\n",
        "\n",
        "# 3. Iterate through each row of the dataframe df\n",
        "for index, row in df.iterrows():\n",
        "    video_id = row['id']\n",
        "    original_video_path_rel = row['video']\n",
        "    deepfake_video_path_rel = row['deepfake']\n",
        "\n",
        "    original_video_path_full = os.path.join(dataset_path, original_video_path_rel)\n",
        "    deepfake_video_path_full = os.path.join(dataset_path, deepfake_video_path_rel)\n",
        "\n",
        "    print(f\"\\nProcessing Video ID: {video_id}\")\n",
        "\n",
        "    # 5. Attempt to open the original video file\n",
        "    cap_original = cv2.VideoCapture(original_video_path_full)\n",
        "\n",
        "    if not cap_original.isOpened():\n",
        "        print(f\"  Error: Could not open original video file {original_video_path_full}. Skipping.\")\n",
        "    else:\n",
        "        print(\"  Processing Original Video...\")\n",
        "        frame_count_orig = 0\n",
        "        # 6. Iterate through its frames\n",
        "        while True:\n",
        "            ret, frame = cap_original.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            # 7. Resize each read frame\n",
        "            resized_frame = cv2.resize(frame, IMG_SIZE)\n",
        "            # 8. Save the resized original frame\n",
        "            frame_filename = f\"video_{video_id}_frame_{frame_count_orig:04d}.jpg\"\n",
        "            frame_filepath = os.path.join(original_frames_dir, frame_filename)\n",
        "            cv2.imwrite(frame_filepath, resized_frame)\n",
        "            frame_count_orig += 1\n",
        "        # 9. Release the original video capture object\n",
        "        cap_original.release()\n",
        "        print(f\"  Extracted and saved {frame_count_orig} frames from original video.\")\n",
        "\n",
        "    # 10. Repeat steps 5-9 for the deepfake video\n",
        "    cap_deepfake = cv2.VideoCapture(deepfake_video_path_full)\n",
        "\n",
        "    if not cap_deepfake.isOpened():\n",
        "        print(f\"  Error: Could not open deepfake video file {deepfake_video_path_full}. Skipping.\")\n",
        "    else:\n",
        "        print(\"  Processing Deepfake Video...\")\n",
        "        frame_count_dfake = 0\n",
        "        while True:\n",
        "            ret, frame = cap_deepfake.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            resized_frame = cv2.resize(frame, IMG_SIZE)\n",
        "            frame_filename = f\"video_{video_id}_frame_{frame_count_dfake:04d}.jpg\"\n",
        "            frame_filepath = os.path.join(deepfake_frames_dir, frame_filename)\n",
        "            cv2.imwrite(frame_filepath, resized_frame)\n",
        "            frame_count_dfake += 1\n",
        "        cap_deepfake.release()\n",
        "        print(f\"  Extracted and saved {frame_count_dfake} frames from deepfake video.\")\n",
        "\n",
        "print(\"\\nFrame extraction and resizing complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directories for extracted frames:\n",
            "- Original: ./deepfake-videos-dataset/extracted_frames/original\n",
            "- Deepfake: ./deepfake-videos-dataset/extracted_frames/deepfake\n",
            "\n",
            "Processing Video ID: 1\n",
            "  Processing Original Video...\n",
            "  Extracted and saved 198 frames from original video.\n",
            "  Processing Deepfake Video...\n",
            "  Extracted and saved 200 frames from deepfake video.\n",
            "\n",
            "Processing Video ID: 2\n",
            "  Processing Original Video...\n",
            "  Extracted and saved 324 frames from original video.\n",
            "  Processing Deepfake Video...\n",
            "  Extracted and saved 135 frames from deepfake video.\n",
            "\n",
            "Processing Video ID: 3\n",
            "  Processing Original Video...\n",
            "  Extracted and saved 277 frames from original video.\n",
            "  Processing Deepfake Video...\n",
            "  Extracted and saved 221 frames from deepfake video.\n",
            "\n",
            "Processing Video ID: 4\n",
            "  Processing Original Video...\n",
            "  Extracted and saved 252 frames from original video.\n",
            "  Processing Deepfake Video...\n",
            "  Extracted and saved 202 frames from deepfake video.\n",
            "\n",
            "Processing Video ID: 5\n",
            "  Error: Could not open original video file ./deepfake-videos-dataset/video/5.mov. Skipping.\n",
            "  Processing Deepfake Video...\n",
            "  Extracted and saved 241 frames from deepfake video.\n",
            "\n",
            "Frame extraction and resizing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b27341d"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Organize the extracted frames and labels (original/deepfake) into a format suitable for input into CNN and LSTM models. This might involve creating sequences of frames for LSTM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65e81946"
      },
      "source": [
        "**Reasoning**:\n",
        "List the extracted frame files, create labels, and store them in a DataFrame for CNN input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "c5cdd88b",
        "outputId": "60baf742-c30f-40db-f91b-5334c87d9885"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 1. List the extracted frame files in the 'original' and 'deepfake' directories\n",
        "original_frames_dir = os.path.join(dataset_path, 'extracted_frames', 'original')\n",
        "deepfake_frames_dir = os.path.join(dataset_path, 'extracted_frames', 'deepfake')\n",
        "\n",
        "original_frame_files = [os.path.join(original_frames_dir, f) for f in os.listdir(original_frames_dir) if f.endswith('.jpg')]\n",
        "deepfake_frame_files = [os.path.join(deepfake_frames_dir, f) for f in os.listdir(deepfake_frames_dir) if f.endswith('.jpg')]\n",
        "\n",
        "# 2. Create labels for each frame\n",
        "original_labels = [0] * len(original_frame_files)\n",
        "deepfake_labels = [1] * len(deepfake_frame_files)\n",
        "\n",
        "# Combine the lists\n",
        "all_frame_files = original_frame_files + deepfake_frame_files\n",
        "all_labels = original_labels + deepfake_labels\n",
        "\n",
        "# 3. Store the file paths and their corresponding labels in a pandas DataFrame\n",
        "frames_df = pd.DataFrame({'frame_path': all_frame_files, 'label': all_labels})\n",
        "\n",
        "# Display the first few rows of the frames DataFrame\n",
        "print(\"DataFrame for CNN input (frame paths and labels):\")\n",
        "display(frames_df.head())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "frames_df.info()\n",
        "\n",
        "# 4. & 5. Prepare data for LSTM (group frames by video ID and create sequences)\n",
        "# This is a conceptual outline as the dataset is very small, making actual sequence creation for training impractical.\n",
        "print(\"\\nPreparing data for LSTM input (conceptual):\")\n",
        "\n",
        "lstm_data = []\n",
        "lstm_labels = []\n",
        "frames_per_sequence = 10 # Example: Define a fixed number of frames per sequence\n",
        "\n",
        "# Group frames by video ID\n",
        "frames_df['video_id'] = frames_df['frame_path'].apply(lambda x: int(os.path.basename(x).split('_')[1]))\n",
        "\n",
        "# Iterate through each video ID\n",
        "for video_id in frames_df['video_id'].unique():\n",
        "    video_frames = frames_df[frames_df['video_id'] == video_id].sort_values('frame_path')\n",
        "    video_label = video_frames['label'].iloc[0] # Assuming all frames from a video have the same label\n",
        "\n",
        "    # Create sequences from frames\n",
        "    # This simple example just takes sequential chunks.\n",
        "    # More sophisticated methods might involve stride, overlapping sequences, etc.\n",
        "    for i in range(0, len(video_frames), frames_per_sequence):\n",
        "        sequence = video_frames.iloc[i:i+frames_per_sequence]\n",
        "        if len(sequence) == frames_per_sequence: # Only include full sequences\n",
        "            # In a real scenario, you would load the images and store them as arrays here\n",
        "            # For this conceptual step, we just store the paths\n",
        "            lstm_data.append(sequence['frame_path'].tolist())\n",
        "            lstm_labels.append(video_label)\n",
        "\n",
        "print(f\"Conceptual LSTM sequences created: {len(lstm_data)}\")\n",
        "if len(lstm_data) > 0:\n",
        "    print(\"First LSTM sequence (frame paths):\")\n",
        "    print(lstm_data[0])\n",
        "    print(\"Corresponding label:\", lstm_labels[0])\n",
        "else:\n",
        "    print(\"No full sequences of length\", frames_per_sequence, \"could be created from the available frames.\")\n",
        "\n",
        "# Note: For actual model training, you would load the images corresponding to these paths\n",
        "# and convert them into a suitable numerical format (e.g., NumPy array) for the LSTM input layer."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame for CNN input (frame paths and labels):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          frame_path  label\n",
              "0  ./deepfake-videos-dataset/extracted_frames/ori...      0\n",
              "1  ./deepfake-videos-dataset/extracted_frames/ori...      0\n",
              "2  ./deepfake-videos-dataset/extracted_frames/ori...      0\n",
              "3  ./deepfake-videos-dataset/extracted_frames/ori...      0\n",
              "4  ./deepfake-videos-dataset/extracted_frames/ori...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b35e4a7-0a78-45ca-b988-ac2f52406314\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frame_path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./deepfake-videos-dataset/extracted_frames/ori...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./deepfake-videos-dataset/extracted_frames/ori...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./deepfake-videos-dataset/extracted_frames/ori...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./deepfake-videos-dataset/extracted_frames/ori...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./deepfake-videos-dataset/extracted_frames/ori...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b35e4a7-0a78-45ca-b988-ac2f52406314')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5b35e4a7-0a78-45ca-b988-ac2f52406314 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5b35e4a7-0a78-45ca-b988-ac2f52406314');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-916dadbb-bc0e-4e03-b9a9-3673fafc3799\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-916dadbb-bc0e-4e03-b9a9-3673fafc3799')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-916dadbb-bc0e-4e03-b9a9-3673fafc3799 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# and convert them into a suitable numerical format (e\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"frame_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"./deepfake-videos-dataset/extracted_frames/original/video_4_frame_0138.jpg\",\n          \"./deepfake-videos-dataset/extracted_frames/original/video_1_frame_0146.jpg\",\n          \"./deepfake-videos-dataset/extracted_frames/original/video_3_frame_0000.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2050 entries, 0 to 2049\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   frame_path  2050 non-null   object\n",
            " 1   label       2050 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 32.2+ KB\n",
            "\n",
            "Preparing data for LSTM input (conceptual):\n",
            "Conceptual LSTM sequences created: 202\n",
            "First LSTM sequence (frame paths):\n",
            "['./deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0000.jpg', './deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0001.jpg', './deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0002.jpg', './deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0003.jpg', './deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0004.jpg', './deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0005.jpg', './deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0006.jpg', './deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0007.jpg', './deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0008.jpg', './deepfake-videos-dataset/extracted_frames/deepfake/video_3_frame_0009.jpg']\n",
            "Corresponding label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba4d4af"
      },
      "source": [
        "**Reasoning**:\n",
        "Define and build the CNN and LSTM model architectures using TensorFlow/Keras, then print their summaries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dc59494c",
        "outputId": "6dee4c3d-b532-4435-9ec7-b7378d20849e"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Get the image size from the previous step\n",
        "# IMG_SIZE is (height, width)\n",
        "IMG_HEIGHT, IMG_WIDTH = IMG_SIZE\n",
        "CHANNELS = 3 # Assuming RGB images\n",
        "\n",
        "# Define the input shape for CNN (single frame)\n",
        "cnn_input_shape = (IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
        "\n",
        "# 1. Define the CNN model architecture\n",
        "def build_cnn_model(input_shape):\n",
        "    \"\"\"Builds a CNN model for frame-based classification.\"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5), # Add dropout for regularization\n",
        "        Dense(1, activation='sigmoid') # Output layer for binary classification\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = build_cnn_model(cnn_input_shape)\n",
        "\n",
        "# Print CNN model summary\n",
        "print(\"CNN Model Architecture:\")\n",
        "cnn_model.summary()\n",
        "\n",
        "# Define parameters for LSTM (sequence of frames)\n",
        "# frames_per_sequence is from the previous data preparation step\n",
        "lstm_input_shape = (frames_per_sequence, IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
        "\n",
        "# 2. Define the LSTM model architecture\n",
        "def build_lstm_model(input_shape):\n",
        "    \"\"\"Builds an LSTM model for sequence-based classification.\"\"\"\n",
        "    model = Sequential([\n",
        "        # Apply Conv2D and MaxPooling to each frame in the sequence\n",
        "        TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape),\n",
        "        TimeDistributed(MaxPooling2D((2, 2))),\n",
        "        TimeDistributed(Conv2D(64, (3, 3), activation='relu')),\n",
        "        TimeDistributed(MaxPooling2D((2, 2))),\n",
        "        TimeDistributed(Conv2D(128, (3, 3), activation='relu')),\n",
        "        TimeDistributed(MaxPooling2D((2, 2))),\n",
        "        TimeDistributed(Flatten()), # Flatten the output of CNN for each frame\n",
        "        LSTM(128, return_sequences=False), # LSTM layer to process sequences\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5), # Add dropout for regularization\n",
        "        Dense(1, activation='sigmoid') # Output layer for binary classification\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the LSTM model\n",
        "lstm_model = build_lstm_model(lstm_input_shape)\n",
        "\n",
        "# Print LSTM model summary\n",
        "print(\"\\nLSTM Model Architecture:\")\n",
        "lstm_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Model Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m409,728\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">409,728</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m503,105\u001b[0m (1.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">503,105</span> (1.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m503,105\u001b[0m (1.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">503,105</span> (1.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LSTM Model Architecture:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m896\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m,     │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │ \u001b[38;5;34m128\u001b[0m)                   │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_5              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_6              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3200\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,704,448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>,     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_5              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_6              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,704,448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,806,017\u001b[0m (6.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,806,017</span> (6.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,806,017\u001b[0m (6.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,806,017</span> (6.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bd73884"
      },
      "source": [
        "## Model training (with caveats)\n",
        "\n",
        "### Subtask:\n",
        "Attempt to train the models on the prepared data. It's crucial to understand that with only 5 samples, this training will primarily serve to demonstrate the process rather than produce a high-performing model. Techniques like transfer learning or significant data augmentation would be necessary for a larger dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49b125d"
      },
      "source": [
        "**Reasoning**:\n",
        "Create NumPy arrays for CNN training data, split the data into training and validation sets, and then train the CNN model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3453dc55",
        "outputId": "879434ed-409e-4fac-810b-809ccbdfdffc"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "\n",
        "# 1. Create NumPy arrays for the CNN training data (frames and labels)\n",
        "print(\"Loading images and preparing data for CNN...\")\n",
        "cnn_data = []\n",
        "cnn_labels = []\n",
        "\n",
        "for index, row in frames_df.iterrows():\n",
        "    img_path = row['frame_path']\n",
        "    label = row['label']\n",
        "\n",
        "    # Load the image using OpenCV\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    if img is not None:\n",
        "        # Ensure the image is in RGB format if it was read as BGR by default\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        # Resize is already done during extraction, but double-check size\n",
        "        if img.shape[0] != IMG_HEIGHT or img.shape[1] != IMG_WIDTH:\n",
        "             img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "\n",
        "        # Normalize pixel values to be between 0 and 1\n",
        "        img = img / 255.0\n",
        "\n",
        "        cnn_data.append(img)\n",
        "        cnn_labels.append(label)\n",
        "    else:\n",
        "        print(f\"Warning: Could not load image from {img_path}. Skipping.\")\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "cnn_data = np.array(cnn_data)\n",
        "cnn_labels = np.array(cnn_labels)\n",
        "\n",
        "print(f\"Loaded {len(cnn_data)} images for CNN training.\")\n",
        "print(f\"CNN data shape: {cnn_data.shape}\")\n",
        "print(f\"CNN labels shape: {cnn_labels.shape}\")\n",
        "\n",
        "# 2. Split the CNN data into training and validation sets\n",
        "# Using a small validation split percentage due to very small dataset size\n",
        "# Note: This split is primarily for demonstration and not reliable evaluation.\n",
        "if len(cnn_data) > 0:\n",
        "    X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
        "        cnn_data, cnn_labels, test_size=0.2, random_state=42, stratify=cnn_labels if len(np.unique(cnn_labels)) > 1 else None\n",
        "    )\n",
        "\n",
        "    print(f\"\\nCNN Training data shape: {X_train_cnn.shape}\")\n",
        "    print(f\"CNN Validation data shape: {X_val_cnn.shape}\")\n",
        "    print(f\"CNN Training labels shape: {y_train_cnn.shape}\")\n",
        "    print(f\"CNN Validation labels shape: {y_val_cnn.shape}\")\n",
        "\n",
        "    # 3. Train the CNN model using the training data and validate on the validation set\n",
        "    print(\"\\nTraining the CNN model...\")\n",
        "    epochs_cnn = 7  # Number of training epochs (adjust based on observation)\n",
        "    batch_size_cnn = 8 # Batch size\n",
        "\n",
        "    # Check if validation set is not empty before training\n",
        "    if X_val_cnn.shape[0] > 0:\n",
        "        cnn_history = cnn_model.fit(\n",
        "            X_train_cnn, y_train_cnn,\n",
        "            epochs=epochs_cnn,\n",
        "            batch_size=batch_size_cnn,\n",
        "            validation_data=(X_val_cnn, y_val_cnn)\n",
        "        )\n",
        "        print(\"\\nCNN model training complete.\")\n",
        "    else:\n",
        "        print(\"\\nSkipping CNN validation as the validation set is empty.\")\n",
        "        cnn_history = cnn_model.fit(\n",
        "            X_train_cnn, y_train_cnn,\n",
        "            epochs=epochs_cnn,\n",
        "            batch_size=batch_size_cnn\n",
        "        )\n",
        "        print(\"\\nCNN model training complete (without validation).\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo CNN data available for training.\")\n",
        "\n",
        "# 4. Prepare the data for LSTM training (conceptual implementation)\n",
        "# This step will load the actual image data for the created sequences.\n",
        "print(\"\\nPreparing data for LSTM...\")\n",
        "\n",
        "lstm_data_actual = []\n",
        "lstm_labels_actual = []\n",
        "\n",
        "# Re-create sequences, this time loading the actual image data\n",
        "# frames_per_sequence is from the previous data preparation step\n",
        "# frames_df['video_id'] = frames_df['frame_path'].apply(lambda x: int(os.path.basename(x).split('_')[1])) # Already added this column before\n",
        "\n",
        "for video_id in frames_df['video_id'].unique():\n",
        "    video_frames = frames_df[frames_df['video_id'] == video_id].sort_values('frame_path')\n",
        "    video_label = video_frames['label'].iloc[0]\n",
        "\n",
        "    for i in range(0, len(video_frames) - frames_per_sequence + 1, 1): # Using stride of 1 for more sequences\n",
        "        sequence = video_frames.iloc[i:i+frames_per_sequence]\n",
        "        sequence_images = []\n",
        "        for _, frame_row in sequence.iterrows():\n",
        "            img_path = frame_row['frame_path']\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                img = img / 255.0\n",
        "                sequence_images.append(img)\n",
        "            else:\n",
        "                 print(f\"Warning: Could not load image from {img_path} for LSTM sequence. Skipping this sequence.\")\n",
        "                 break # Skip the entire sequence if any frame is missing\n",
        "\n",
        "        if len(sequence_images) == frames_per_sequence:\n",
        "            lstm_data_actual.append(sequence_images)\n",
        "            lstm_labels_actual.append(video_label)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "lstm_data_actual = np.array(lstm_data_actual)\n",
        "lstm_labels_actual = np.array(lstm_labels_actual)\n",
        "\n",
        "print(f\"\\nPrepared {len(lstm_data_actual)} sequences for LSTM training.\")\n",
        "print(f\"LSTM data shape: {lstm_data_actual.shape}\")\n",
        "print(f\"LSTM labels shape: {lstm_labels_actual.shape}\")\n",
        "\n",
        "\n",
        "# 5. Create corresponding labels for the LSTM sequences (already done in step 4)\n",
        "# 6. Split the LSTM data into training and validation sets\n",
        "if len(lstm_data_actual) > 0:\n",
        "    X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = train_test_split(\n",
        "        lstm_data_actual, lstm_labels_actual, test_size=0.2, random_state=42, stratify=lstm_labels_actual if len(np.unique(lstm_labels_actual)) > 1 else None\n",
        "    )\n",
        "\n",
        "    print(f\"\\nLSTM Training data shape: {X_train_lstm.shape}\")\n",
        "    print(f\"LSTM Validation data shape: {X_val_lstm.shape}\")\n",
        "    print(f\"LSTM Training labels shape: {y_train_lstm.shape}\")\n",
        "    print(f\"LSTM Validation labels shape: {y_val_lstm.shape}\")\n",
        "\n",
        "    # 7. Train the LSTM model using the training data and validate on the validation set\n",
        "    print(\"\\nTraining the LSTM model...\")\n",
        "    epochs_lstm = 10 # Number of training epochs\n",
        "    batch_size_lstm = 4 # Batch size (smaller for LSTM due to sequence data)\n",
        "\n",
        "    if X_val_lstm.shape[0] > 0:\n",
        "        lstm_history = lstm_model.fit(\n",
        "            X_train_lstm, y_train_lstm,\n",
        "            epochs=epochs_lstm,\n",
        "            batch_size=batch_size_lstm,\n",
        "            validation_data=(X_val_lstm, y_val_lstm)\n",
        "        )\n",
        "        print(\"\\nLSTM model training complete.\")\n",
        "    else:\n",
        "         print(\"\\nSkipping LSTM validation as the validation set is empty.\")\n",
        "         lstm_history = lstm_model.fit(\n",
        "            X_train_lstm, y_train_lstm,\n",
        "            epochs=epochs_lstm,\n",
        "            batch_size=batch_size_lstm\n",
        "         )\n",
        "         print(\"\\nLSTM model training complete (without validation).\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo LSTM data available for training.\")\n",
        "\n",
        "# 8. Print a brief summary of the training process for both models\n",
        "print(\"\\n--- Training Summary ---\")\n",
        "\n",
        "if 'cnn_history' in locals():\n",
        "    print(\"\\nCNN Model Training:\")\n",
        "    print(f\"  Epochs trained: {epochs_cnn}\")\n",
        "    print(f\"  Last training accuracy: {cnn_history.history['accuracy'][-1]:.4f}\")\n",
        "    if X_val_cnn.shape[0] > 0:\n",
        "        print(f\"  Last validation accuracy: {cnn_history.history['val_accuracy'][-1]:.4f}\")\n",
        "    print(\"  Observations: Training progress was made, but performance metrics on the validation set (if available) should be interpreted with extreme caution due to the tiny dataset size.\")\n",
        "\n",
        "if 'lstm_history' in locals():\n",
        "    print(\"\\nLSTM Model Training:\")\n",
        "    print(f\"  Epochs trained: {epochs_lstm}\")\n",
        "    print(f\"  Last training accuracy: {lstm_history.history['accuracy'][-1]:.4f}\")\n",
        "    if X_val_lstm.shape[0] > 0:\n",
        "        print(f\"  Last validation accuracy: {lstm_history.history['val_accuracy'][-1]:.4f}\")\n",
        "    print(\"  Observations: Similar to the CNN, training progress was made, but the results are not indicative of real-world performance due to the limited data. The models may have simply memorized the few available samples.\")\n",
        "\n",
        "print(\"\\nImportant Note: The training results obtained with this extremely small dataset (5 samples) are for demonstration purposes only. They do not represent the performance of these models on a larger, more realistic dataset. Overfitting is highly probable. Techniques like transfer learning and extensive data augmentation would be essential for training on a sufficiently large dataset.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images and preparing data for CNN...\n",
            "Loaded 2050 images for CNN training.\n",
            "CNN data shape: (2050, 56, 56, 3)\n",
            "CNN labels shape: (2050,)\n",
            "\n",
            "CNN Training data shape: (1640, 56, 56, 3)\n",
            "CNN Validation data shape: (410, 56, 56, 3)\n",
            "CNN Training labels shape: (1640,)\n",
            "CNN Validation labels shape: (410,)\n",
            "\n",
            "Training the CNN model...\n",
            "Epoch 1/7\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.5603 - loss: 0.6858 - val_accuracy: 0.5878 - val_loss: 0.5925\n",
            "Epoch 2/7\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6232 - loss: 0.5921 - val_accuracy: 0.6585 - val_loss: 0.5743\n",
            "Epoch 3/7\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6567 - loss: 0.5421 - val_accuracy: 0.6293 - val_loss: 0.5465\n",
            "Epoch 4/7\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6859 - loss: 0.4971 - val_accuracy: 0.7634 - val_loss: 0.3790\n",
            "Epoch 5/7\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7687 - loss: 0.3928 - val_accuracy: 0.8390 - val_loss: 0.3093\n",
            "Epoch 6/7\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8148 - loss: 0.3244 - val_accuracy: 0.7927 - val_loss: 0.2911\n",
            "Epoch 7/7\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8373 - loss: 0.2820 - val_accuracy: 0.8634 - val_loss: 0.2363\n",
            "\n",
            "CNN model training complete.\n",
            "\n",
            "Preparing data for LSTM...\n",
            "\n",
            "Prepared 2005 sequences for LSTM training.\n",
            "LSTM data shape: (2005, 10, 56, 56, 3)\n",
            "LSTM labels shape: (2005,)\n",
            "\n",
            "LSTM Training data shape: (1604, 10, 56, 56, 3)\n",
            "LSTM Validation data shape: (401, 10, 56, 56, 3)\n",
            "LSTM Training labels shape: (1604,)\n",
            "LSTM Validation labels shape: (401,)\n",
            "\n",
            "Training the LSTM model...\n",
            "Epoch 1/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.9836 - loss: 0.0294 - val_accuracy: 1.0000 - val_loss: 5.2764e-07\n",
            "Epoch 2/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.0540e-04 - val_accuracy: 1.0000 - val_loss: 4.9880e-08\n",
            "Epoch 3/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 5.7226e-05 - val_accuracy: 1.0000 - val_loss: 6.8836e-09\n",
            "Epoch 4/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 3.9051e-05 - val_accuracy: 1.0000 - val_loss: 2.0896e-09\n",
            "Epoch 5/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 4.8049e-05 - val_accuracy: 1.0000 - val_loss: 5.1188e-10\n",
            "Epoch 6/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.4849e-06 - val_accuracy: 1.0000 - val_loss: 3.6031e-10\n",
            "Epoch 7/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 2.3758e-06 - val_accuracy: 1.0000 - val_loss: 2.3683e-10\n",
            "Epoch 8/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.7432e-05 - val_accuracy: 1.0000 - val_loss: 4.8381e-11\n",
            "Epoch 9/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 9.3174e-06 - val_accuracy: 1.0000 - val_loss: 1.1312e-11\n",
            "Epoch 10/10\n",
            "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 2.8602e-06 - val_accuracy: 1.0000 - val_loss: 6.7907e-12\n",
            "\n",
            "LSTM model training complete.\n",
            "\n",
            "--- Training Summary ---\n",
            "\n",
            "CNN Model Training:\n",
            "  Epochs trained: 7\n",
            "  Last training accuracy: 0.8463\n",
            "  Last validation accuracy: 0.8634\n",
            "  Observations: Training progress was made, but performance metrics on the validation set (if available) should be interpreted with extreme caution due to the tiny dataset size.\n",
            "\n",
            "LSTM Model Training:\n",
            "  Epochs trained: 10\n",
            "  Last training accuracy: 1.0000\n",
            "  Last validation accuracy: 1.0000\n",
            "  Observations: Similar to the CNN, training progress was made, but the results are not indicative of real-world performance due to the limited data. The models may have simply memorized the few available samples.\n",
            "\n",
            "Important Note: The training results obtained with this extremely small dataset (5 samples) are for demonstration purposes only. They do not represent the performance of these models on a larger, more realistic dataset. Overfitting is highly probable. Techniques like transfer learning and extensive data augmentation would be essential for training on a sufficiently large dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3165a0"
      },
      "source": [
        "**Reasoning**:\n",
        "Continue training the LSTM model as the previous output shows the training starting but not completed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1564c02c"
      },
      "source": [
        "## Evaluation (limited)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained CNN and LSTM models on the validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5da0fc41"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained CNN and LSTM models using appropriate metrics on the validation set. Due to the extremely small dataset size, the evaluation results will not be a reliable indicator of real-world performance but will demonstrate the evaluation process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4caca75",
        "outputId": "8274c678-ce48-4268-f3b5-823d159a46a0"
      },
      "source": [
        "# Evaluate the CNN model\n",
        "print(\"Evaluating CNN model on the validation set...\")\n",
        "if 'X_val_cnn' in locals() and X_val_cnn.shape[0] > 0:\n",
        "    loss_cnn, accuracy_cnn = cnn_model.evaluate(X_val_cnn, y_val_cnn, verbose=0)\n",
        "    print(f\"CNN Validation Loss: {loss_cnn:.4f}\")\n",
        "    print(f\"CNN Validation Accuracy: {accuracy_cnn:.4f}\")\n",
        "else:\n",
        "    print(\"No CNN validation data available for evaluation.\")\n",
        "\n",
        "# Evaluate the LSTM model\n",
        "print(\"\\nEvaluating LSTM model on the validation set...\")\n",
        "if 'X_val_lstm' in locals() and X_val_lstm.shape[0] > 0:\n",
        "    loss_lstm, accuracy_lstm = lstm_model.evaluate(X_val_lstm, y_val_lstm, verbose=0)\n",
        "    print(f\"LSTM Validation Loss: {loss_lstm:.4f}\")\n",
        "    print(f\"LSTM Validation Accuracy: {accuracy_lstm:.4f}\")\n",
        "else:\n",
        "    print(\"No LSTM validation data available for evaluation.\")\n",
        "\n",
        "print(\"\\nImportant Note: The evaluation results on this extremely small dataset are not reliable indicators of real-world model performance due to severe overfitting.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating CNN model on the validation set...\n",
            "CNN Validation Loss: 0.2363\n",
            "CNN Validation Accuracy: 0.8634\n",
            "\n",
            "Evaluating LSTM model on the validation set...\n",
            "LSTM Validation Loss: 0.0000\n",
            "LSTM Validation Accuracy: 1.0000\n",
            "\n",
            "Important Note: The evaluation results on this extremely small dataset are not reliable indicators of real-world model performance due to severe overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7872f07e",
        "outputId": "6572c4a4-e4e3-45e9-e492-47dc11c24352"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the directory to save the models\n",
        "model_save_dir = './saved_models'\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# Save the CNN model\n",
        "cnn_model_path = os.path.join(model_save_dir, 'cnn_deepfake_detector.h5')\n",
        "try:\n",
        "    cnn_model.save(cnn_model_path)\n",
        "    print(f\"CNN model saved successfully to {cnn_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving CNN model: {e}\")\n",
        "\n",
        "\n",
        "# Save the LSTM model\n",
        "lstm_model_path = os.path.join(model_save_dir, 'lstm_deepfake_detector5.h5')\n",
        "try:\n",
        "    lstm_model.save(lstm_model_path)\n",
        "    print(f\"LSTM model saved successfully to {lstm_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving LSTM model: {e}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model saved successfully to ./saved_models/cnn_deepfake_detector.h5\n",
            "LSTM model saved successfully to ./saved_models/lstm_deepfake_detector5.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p3s4s9T4zgEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define the path to the combined model file\n",
        "model_save_dir = './saved_models'\n",
        "combined_model_path = os.path.join(model_save_dir, 'deepfake_detection_models.h5')\n",
        "\n",
        "# Check if the combined model file exists\n",
        "if os.path.exists(combined_model_path):\n",
        "    # Load the models from the combined HDF5 file\n",
        "    try:\n",
        "        with h5py.File(combined_model_path, 'r') as f:\n",
        "            # Load the CNN model\n",
        "            cnn_model_loaded = load_model(f.get('cnn_model'))\n",
        "            print(\"CNN model loaded successfully.\")\n",
        "\n",
        "            # Load the LSTM model\n",
        "            lstm_model_loaded = load_model(f.get('lstm_model'))\n",
        "            print(\"LSTM model loaded successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models from {combined_model_path}: {e}\")\n",
        "else:\n",
        "    print(f\"Combined model file not found at {combined_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emKUfkXfzP0G",
        "outputId": "8c756b28-c04c-4447-d947-10ba34dc7117"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading models from ./saved_models/deepfake_detection_models.h5: stat: path should be string, bytes, os.PathLike or integer, not Group\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20f7f6fa"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The dataset contains four columns: 'id', 'deepfake', 'image', and 'video'.\n",
        "* Each row in the dataset represents a single sample, where the 'id' links together the original video ('video'), its corresponding deepfake version ('deepfake'), and a related image file ('image').\n",
        "* The file paths provided in the DataFrame for 'video', 'deepfake', and 'image' columns correspond to actual files within the specified dataset directory.\n",
        "* Analysis of video characteristics using OpenCV revealed variations in resolution, frame rate, and duration between original and deepfake video pairs.\n",
        "* One original video file (.mov) could not be opened by OpenCV during the analysis.\n",
        "* The dataset contains only 5 samples, which is extremely small for training a robust deepfake detection model.\n",
        "\n",
        "### Model Training and Evaluation Observations\n",
        "\n",
        "* We successfully performed preprocessing by extracting and resizing frames from the videos.\n",
        "* We built and trained both CNN and LSTM models for deepfake detection.\n",
        "* Due to the extremely small dataset size (5 samples), the models likely overfit to the training data, resulting in very high accuracy on the validation set (e.g., 100% for LSTM). These results are not reliable indicators of how the models would perform on unseen, real-world data.\n",
        "\n",
        "### Conclusion and Next Steps\n",
        "\n",
        "* This exercise demonstrated the typical workflow for preparing video data and building deep learning models for a classification task.\n",
        "* However, the primary limitation is the dataset size. To build a truly effective deepfake detection model, a significantly larger and more diverse dataset would be required.\n",
        "* With a larger dataset, the next steps would involve more rigorous model training, potentially using techniques like transfer learning, more extensive data augmentation, hyperparameter tuning, and more robust evaluation methods (e.g., cross-validation).\n",
        "* For this specific small dataset, its value lies primarily in understanding the data structure, practicing basic preprocessing steps, and demonstrating the model building process, rather than in achieving a high-performing model."
      ]
    }
  ]
}